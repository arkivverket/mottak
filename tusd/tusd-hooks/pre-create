#!/usr/bin/env python3
import os                               # for getenv
import sys
import json
import psycopg2
import psycopg2.extras
import string
import subprocess
# For python 3.6
from subprocess import PIPE
import random
import re  # for parsing the DBSTRING

import logging

from dotenv import load_dotenv
load_dotenv()

# Todo:
# - validate filename (should be $uuid.tar)
# - check that the uploader URL has not been tampered with


# Allow for override (for dev purposes)
WORKFLOW = os.getenv('WORKFLOW', default='/srv/argo-mottak-DAG.yaml')

# Return codes.
UUIDERROR = 1  # invalid UUID
DBERROR = 10
JSONERROR = 11
IOERROR = 12
USAGEERROR = 13
ARGOERROR = 14
UNKNOWNUUID = 15
UNKNOWNIID = 16


# There needs to be a envir variable called DBSTRING
# it is on the following format:

# 'pgsql:host=10.0.0.0;dbname=foo;user=myuser;password=verydull'


def create_db_access(dbstring):
    """Create a psycopg2 compatible object from the connection string.
    The string is from PHP and we reuse it here
    """
    mystr = dbstring[6:]
    mystr = mystr.rstrip()
    d = dict(re.findall(r'(\w+)=([^;]+);?', mystr))
    # Validate dbstring:
    for key in ['user', 'password', 'host', 'dbname']:
        if key not in d.keys():
            logging.error('%s not found in DBSTRING' % key)
            exit(DBERROR)
    return d


def my_connect(conn_info):
    try:
        connection = psycopg2.connect(user=conn_info['user'],
                                      host=conn_info['host'],
                                      dbname=conn_info['dbname'],
                                      password=conn_info['password'],
                                      connect_timeout=10)

    except (Exception, psycopg2.Error) as error:
        logging.error(f"Error while connecting to PostgreSQL: {error}")
        exit(DBERROR)
    finally:
        return connection


def my_disconnect(conn):
    conn.close()


def read_tusd_event(step):
    try:
        data = json.load(sys.stdin)
    except ValueError as e:
        logging.error(f"Error parsing JSON({step}): {e}")
        exit(JSONERROR)
    # Enable this when debugging the events. It dumps the input to /tmp so you can re-run the hook with stdin.
    with open(f'/tmp/json-event-{step}.json', 'w') as event_file:
        json.dump(data, event_file)

    return data


def get_metadata(conn, iid):
    try:
        dict_cursor = conn.cursor(
            cursor_factory=psycopg2.extras.RealDictCursor)
        dict_cursor.execute('SELECT invitations.id, uuid, checksum, is_sensitive, name, email, type '
                            'FROM invitations, archive_types '
                            'WHERE archive_type_id=archive_types.id '
                            'AND invitations.id=%s', (iid,))
        rec = dict_cursor.fetchall()
    except psycopg2.Error as e:
        logging.error(f'SQL Query error: {e}')
        exit(DBERROR)

    if len(rec) == 0:
        return None
    else:
        return rec[0]


def randomString(string_length=32):
    """Generate a random string of fixed length """
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(string_length))


def create_param_file(tmpfile, metadata, data):
    """ Create a metadata JSON-file for Argo to ingest.
        This file contains the workflow parameters referenced in the workflow.
    
        Parameters:
        -----------
         tmpfile : str
         metadata : dict
         data : dict (data object given from tusd)
     """
    # define en workflow parameters
    # If we expand support to other objectstore this needs to pick up the relevant env variables
    # and inject them into the workflow.
    env = ['OBJECTSTORE',     # type of objectstore: gcs, s3, azure etc.
           'GOOGLE_ACCOUNT',  # for gooogle
           'ENDPOINT', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'BUCKET', 'AWS_REGION',  # for AWS
           'MAILGUN_DOMAIN',
           'MAILGUN_API_KEY', ]
    with open(tmpfile, "w") as pfile:
        for key in env:
            if (env[key]):
                print(key+':', os.getenv(key), file=pfile)
        print('UUID:', metadata['uuid'], file=pfile)
        # Note: tusd gives the file a random name.
        print('OBJECT:', data['Upload']['Storage']['Key'], file=pfile)
        print('CHECKSUM:', metadata['checksum'], file=pfile)
        print('ARCHIEVE_TYPE:', metadata['type'], file=pfile)
        print('NAME:', metadata['name'],  file=pfile)
        print('EMAIL:', metadata['email'], file=pfile)


def argo_submit(paramfile):
    """ Submit a job to argo. Takes a YAML file as parameter """
    argocmd = ["argo", "submit", "--parameter-file", paramfile, WORKFLOW]
    print("Argo cmd line:", argocmd)
    # Python 3.7
    #submit = subprocess.run(argocmd, timeout=10, capture_output=True)
    # For Python 3.6 (tusd docker image has python3.6)
    submit = subprocess.run(argocmd, timeout=10, stdout=PIPE, stderr=PIPE)
    if not (submit.returncode == 0):
        logging.error("Argo submit failed")
        if submit.stderr:
            logging.error(f"Stderr: {submit.stderr.decode('utf-8')}")
        if submit.stdout:
            logging.error(f"Stdout: {submit.stdout.decode('utf-8')}")
        exit(ARGOERROR)


########################################################

# We use the same source for both pre-create and post-finish hook
# This identifies it
my_name = os.path.basename(__file__)

data = read_tusd_event(step=my_name)
param_file = '/tmp/argo-input-' + randomString()

if not (os.getenv('DBSTRING')):
    logging.error("DBSTRING environment variable not set")
    exit(USAGEERROR)

try:
    iid = data["Upload"]["MetaData"]["invitation_id"]
except:
    logging.error(f"Could not find invitation_id in JSON: {iid}")
    exit(UNKNOWNIID)


# print(f"Getting metadata for IID {iid}")

connection = my_connect(create_db_access(os.getenv('DBSTRING')))

metadata = get_metadata(connection, iid)

# print(metadata)

uuid = metadata['uuid']


# This is the pre-create hook. The only concern here is to validate the UUID
if (my_name == 'pre-create'):
    if not (uuid == metadata['uuid']):
        logging.error('Unknown UUID')
        exit(UUIDERROR)
    else:
        exit(0)

# We assume that we're the post-create hook and we create an input-file for argo
# and submit the workflow into argo.

# Verify that we have a filename:
try:
    filename = data['Upload']['Storage']['Key']
    print(f"Filename is {filename}")
except:
    logging.error("Could not key/filename in JSON. Dumping JSON:")
    logging.error(json.dump(data))
    exit(JSONERROR)


if ((metadata) and ('uuid' in metadata)):
    create_param_file(param_file, metadata, data)
    argo_submit(param_file)
    os.remove(param_file)
    my_disconnect(connection)

    exit(0)
else:
    print("Unknown UUID:" + uuid)
    exit(UUIDERROR)


# Stick these things as k8s secrets and dump them into the input file.
# Then kick off argo.


# ENDPOINT: https://storage.googleapis.com
# AWS_ACCESS_KEY_ID:
# AWS_SECRET_ACCESS_KEY:
# BUCKET: pilot-spool
# OBJECT: 'uuid.tar'
# CHECKSUM: 2afeec307b0573339b3292e27e7971b5b040a5d7e8f7432339cae2fcd0eb936a
# REGION_NAME: us-east-1
# ARCHIEVE_TYPE: noark5
