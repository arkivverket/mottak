#!/usr/bin/env python3
from __future__ import print_function   # for eprint
import os                               # for getenv
import sys
import json
import psycopg2
import psycopg2.extras
import string
import subprocess
import random
import re  # for parsing the DBSTRING

from dotenv import load_dotenv
load_dotenv()

# Todo:
# - validate filename (should be $uuid.tar)
# - check that the uploader URL has not been tampered with


# Allow for override (for dev purposes)
WORKFLOW = os.getenv('WORKFLOW', default='/srv/argo-mottak-DAG.yaml')

# Return codes.
UUIDERROR = 1  # invalid UUID
DBERROR = 10
JSONERROR = 11
IOERROR = 12
USAGEERROR = 13
ARGOERROR = 14
UNKNOWNUUID = 15
UNKNOWNIID = 16


def eprint(*args, **kwargs):
    """ Print to stderr """
    print(*args, file=sys.stderr, **kwargs)

# There needs to be a envir variable called DBSTRING
# it is on the following format:

# 'pgsql:host=10.0.0.0;dbname=foo;user=myuser;password=verydull'


def create_db_access(dbstring):
    """Create a psycopg2 compatible object from the connection string.
    The string is from PHP and we reuse it here
    """
    mystr = dbstring[6:]
    mystr = mystr.rstrip()
    d = dict(re.findall(r'(\w+)=([^;]+);?', mystr))
    # Validate dbstring:
    for key in ['user', 'password', 'host', 'dbname']:
        if key not in d.keys():
            eprint('%s not found in DBSTRING' % key)
            exit(DBERROR)
    return d


def my_connect(conn_info):
    try:
        connection = psycopg2.connect(user=conn_info['user'],
                                      host=conn_info['host'],
                                      dbname=conn_info['dbname'],
                                      password=conn_info['password'],
                                      connect_timeout=10)

    except (Exception, psycopg2.Error) as error:
        eprint("Error while connecting to PostgreSQL:", error)
        exit(DBERROR)
    finally:
        return connection


def my_disconnect(conn):
    conn.close()


def read_tusd_event():
    try:
        data = json.load(sys.stdin)
    except ValueError as e:
        eprint("Error parsing JSON:", e)
        exit(JSONERROR)
    return data


def get_metadata(conn, iid):
    try:
        dict_cursor = conn.cursor(
            cursor_factory=psycopg2.extras.RealDictCursor)
        dict_cursor.execute('SELECT invitations.id, uuid, checksum, is_sensitive, name, email, type '
                            'FROM invitations, archive_types '
                            'WHERE archive_type_id=archive_types.id '
                            'AND invitations.id=%s', (iid,))
        rec = dict_cursor.fetchall()
    except psycopg2.Error as e:
        eprint('SQL Query error:', e)
        exit(DBERROR)

    if len(rec) == 0:
        return None
    else:
        return rec[0]


def randomString(stringLength=10):
    """Generate a random string of fixed length """
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(stringLength))


def create_param_file(tmpfile, metadata, data):
    """ Create a metadata JSON-file for Argo to ingest """
    env = ['ENDPOINT', 'AWS_ACCESS_KEY_ID',
           'AWS_SECRET_ACCESS_KEY', 'BUCKET', 'AWS_REGION', 
           'MAILGUN_DOMAIN',
           'MAILGUN_API_KEY']
    with open(tmpfile, "w") as pfile:
        for key in env:
            print(key+':', os.getenv(key), file=pfile)
        print('UUID:', metadata['uuid'], file=pfile)
        print('OBJECT:', metadata['uuid'] + '.tar', file=pfile)
        print('CHECKSUM:', metadata['checksum'], file=pfile)
        print('ARCHIEVE_TYPE:', metadata['type'], file=pfile)
        print('NAME:', metadata['name'],  file=pfile)
        print('EMAIL:', metadata['email'], file=pfile)



def argo_submit(paramfile):
    """ Submit a job to argo. Takes a JSON file as parameter """
    argocmd = ["argo", "submit", "--parameter-file", paramfile, WORKFLOW]
    print("Argo cmd line:", argocmd)
    submit = subprocess.run(argocmd, timeout=10, capture_output=True)
    if not (submit.returncode == 0):
        eprint("Argo submit failed")
        if submit.stderr:
            eprint("Stderr: ", submit.stderr.decode('utf-8'))
        if submit.stdout:
            eprint("Stdout: ", submit.stdout.decode('utf-8'))
        exit(ARGOERROR)


########################################################

# We use the same source for both pre-create and post-finish hook
# This identifies it
my_name = os.path.basename(__file__)

data = read_tusd_event()
param_file = '/tmp/argo-input-' + randomString()

if not (os.getenv('DBSTRING')):
    eprint("DBSTRING environment variable not set")
    exit(USAGEERROR)

try:
    iid = data["Upload"]["MetaData"]["invitation_id"]
except:
    eprint("Could not find invitation_id in JSON:", iid)
    exit(UNKNOWNIID)

# print(f"Getting metadata for IID {iid}")

connection = my_connect(create_db_access(os.getenv('DBSTRING')))

metadata = get_metadata(connection, iid)

uuid = metadata['uuid']


# This is the pre-create hook. The only concern here is to validate the UUID
if (my_name == 'pre-create'):
    if not (uuid == metadata['uuid']):
        eprint('Unknown UUID')
        exit(UUIDERROR)
    else:
        exit(0)

# We assume that we're the post-create hook and we create an input-file for argo
# and submit the workflow into argo.

if ((metadata) and ('uuid' in metadata)):
    create_param_file(param_file, metadata, data)
    argo_submit(param_file)
    os.remove(param_file)
    my_disconnect(connection)

    exit(0)
else:
    print("Unknown UUID:" + uuid)
    exit(UUIDERROR)


# Stick these things as k8s secrets and dump them into the input file.
# Then kick off argo.


# ENDPOINT: https://storage.googleapis.com
# AWS_ACCESS_KEY_ID:
# AWS_SECRET_ACCESS_KEY:
# BUCKET: pilot-spool
# OBJECT: 'uuid.tar'
# CHECKSUM: 2afeec307b0573339b3292e27e7971b5b040a5d7e8f7432339cae2fcd0eb936a
# REGION_NAME: us-east-1
# ARCHIEVE_TYPE: noark5
